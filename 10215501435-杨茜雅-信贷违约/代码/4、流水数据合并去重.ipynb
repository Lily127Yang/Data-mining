{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1、流水数据文件夹下的三张表合并&去重"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为什么要把流水数据合起来用？事实证明就算是把流水数据下的三张表都用上了，也有一部分的merchant_id找不到任何的流水记录，我们当然希望尽可能地不遗漏所有的流水信息，而三张表的信息量肯定比一张表大（注意去重交易id）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "流水数据/table_lend_transaction_final.txt 的数据量: 3590059\n",
      "流水数据/table_loss_transaction_testdata.txt 的数据量: 2404994\n",
      "流水数据/table_loss_transaction_trainingdata.txt 的数据量: 2309665\n",
      "完全一致的重复数据量: 940647\n",
      "去重后的总数据量: 7295027\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "file_paths = [\n",
    "    '流水数据/table_lend_transaction_final.txt',\n",
    "    '流水数据/table_loss_transaction_testdata.txt',\n",
    "    '流水数据/table_loss_transaction_trainingdata.txt'\n",
    "]\n",
    "\n",
    "# 读取数据\n",
    "dataframes = [pd.read_csv(file) for file in file_paths]\n",
    "\n",
    "# 打印各个文件的数据量\n",
    "for df, path in zip(dataframes, file_paths):\n",
    "    print(f\"{path} 的数据量: {df.shape[0]}\")\n",
    "\n",
    "# 合并数据以寻找重复项\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# 查找完全一致的重复行\n",
    "duplicates = combined_df[combined_df.duplicated(keep=False)]\n",
    "unique_duplicates = duplicates.drop_duplicates()\n",
    "\n",
    "# 打印完全一致的重复数据量\n",
    "print(f\"完全一致的重复数据量: {unique_duplicates.shape[0]}\")\n",
    "\n",
    "# 删除完全一致的重复行，只保留一个\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "# 打印去重后的总数据量\n",
    "print(f\"去重后的总数据量: {combined_df.shape[0]}\")\n",
    "\n",
    "# 如果需要，可以保存合并后的数据\n",
    "# combined_df.to_csv('流水数据/combined_data_cleaned.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "流水数据/table_lend_transaction_final.txt 的数据量为: 3590059行\n",
      "流水数据/table_loss_transaction_testdata.txt 的数据量为: 2404994行\n",
      "流水数据/table_loss_transaction_trainingdata.txt 的数据量为: 2309665行\n",
      "合并后的总数据量为: 8304718行\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "file_paths = [\n",
    "    '流水数据/table_lend_transaction_final.txt',\n",
    "    '流水数据/table_loss_transaction_testdata.txt',\n",
    "    '流水数据/table_loss_transaction_trainingdata.txt'\n",
    "]\n",
    "\n",
    "# 读取数据并打印各个文件的数据量\n",
    "dataframes = []\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file)\n",
    "    print(f\"{file} 的数据量为: {df.shape[0]}行\")\n",
    "    dataframes.append(df)\n",
    "\n",
    "# 合并数据\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print(f\"合并后的总数据量为: {combined_df.shape[0]}行\")\n",
    "\n",
    "# 保存合并后的数据到新文件\n",
    "combined_df.to_csv('流水数据/combined_data.txt', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据量: 8304718行\n",
      "去重后的数据量: 7295027行\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "file_path = '流水数据/combined_data.txt'\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 原始数据量打印\n",
    "print(f\"原始数据量: {df.shape[0]}行\")\n",
    "\n",
    "# 删除重复行，只保留一个\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 打印去重后的数据量\n",
    "print(f\"去重后的数据量: {df.shape[0]}行\")\n",
    "\n",
    "# 保存去重后的数据到新文件\n",
    "df.to_csv('流水数据/combined_data_unique.txt', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2、查看type和status字段有多少不一样的取值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status字段有 13 个不同的取值。\n",
      "status各取值及其数量:\n",
      "2000    6931692\n",
      "2001     351935\n",
      "2107       4794\n",
      "1001       2954\n",
      "2102       1938\n",
      "2104       1073\n",
      "0           301\n",
      "2101        192\n",
      "1002         95\n",
      "2108         29\n",
      "2109         12\n",
      "2106          7\n",
      "2002          5\n",
      "Name: status, dtype: int64\n",
      "type字段有 3 个不同的取值。\n",
      "type各取值及其数量:\n",
      "30    7290543\n",
      "10       3373\n",
      "11       1111\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载数据\n",
    "data_path = '流水数据/combined_data_unique.txt'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 获取status字段的所有唯一取值并统计每个唯一取值的出现次数\n",
    "unique_status_values = data['status'].unique()\n",
    "status_value_counts = data['status'].value_counts()\n",
    "\n",
    "# 获取type字段的所有唯一取值并统计每个唯一取值的出现次数\n",
    "unique_type_values = data['type'].unique()\n",
    "type_value_counts = data['type'].value_counts()\n",
    "\n",
    "# 打印status字段的结果\n",
    "print(f\"status字段有 {len(unique_status_values)} 个不同的取值。\")\n",
    "print(\"status各取值及其数量:\")\n",
    "print(status_value_counts)\n",
    "\n",
    "# 打印type字段的结果\n",
    "print(f\"type字段有 {len(unique_type_values)} 个不同的取值。\")\n",
    "print(\"type各取值及其数量:\")\n",
    "print(type_value_counts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、 只保留type为30和status为2000的字段，然后删除包含ctime的三个字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据大小：7295027 行\n",
      "status不为2000的行有：363335 行\n",
      "删除status不为2000后的数据大小：6931692 行\n",
      "type字段为30的行有：6927255 行\n",
      "删除type字段不为30的行后的数据大小：6927255 行\n",
      "数据处理完成，并保存到 '流水数据/clean.txt'。\n",
      "包含 '\\N' 的总行数: 11261\n",
      "每个字段包含 '\\N' 的个数:\n",
      "paid_amount    11261\n",
      "dtype: int64\n",
      "原始数据行数: 6927255\n",
      "删除包含 '\\N' 的行后的数据行数: 6915994\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载数据\n",
    "data_path = '流水数据/combined_data_unique.txt'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 显示原始数据大小\n",
    "original_size = data.shape[0]\n",
    "print(f\"原始数据大小：{original_size} 行\")\n",
    "\n",
    "# 统计并过滤status字段\n",
    "status_not_2000_count = data[data['status'] != 2000].shape[0]\n",
    "data = data[data['status'] == 2000]\n",
    "print(f\"status不为2000的行有：{status_not_2000_count} 行\")\n",
    "print(f\"删除status不为2000后的数据大小：{data.shape[0]} 行\")\n",
    "\n",
    "# 统计并过滤type字段\n",
    "type_30_count = data[data['type'] == 30].shape[0]\n",
    "data = data[data['type'] == 30]\n",
    "print(f\"type字段为30的行有：{type_30_count} 行\")\n",
    "print(f\"删除type字段不为30的行后的数据大小：{data.shape[0]} 行\")\n",
    "\n",
    "# 删除不必要的字段\n",
    "data.drop(['type', 'status', 'ctime'], axis=1, inplace=True)\n",
    "\n",
    "# 保存清洗后的数据\n",
    "output_file_path = '流水数据/clean.txt'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "print(f\"数据处理完成，并保存到 '{output_file_path}'。\")\n",
    "\n",
    "# 重新加载清洗后的数据\n",
    "data = pd.read_csv(output_file_path)\n",
    "\n",
    "# 检查每个字段是否包含 \\N\n",
    "contains_N_per_column = data.apply(lambda x: x.astype(str).str.contains(r'\\\\N'))\n",
    "\n",
    "# 统计包含 \\N 的情况\n",
    "count_N_per_column = contains_N_per_column.sum()\n",
    "total_N_rows = contains_N_per_column.any(axis=1).sum()\n",
    "print(f\"包含 '\\\\N' 的总行数: {total_N_rows}\")\n",
    "print(\"每个字段包含 '\\\\N' 的个数:\")\n",
    "print(count_N_per_column[count_N_per_column > 0])\n",
    "\n",
    "# 删除包含 \\N 的行\n",
    "data_cleaned = data[~contains_N_per_column.any(axis=1)]\n",
    "data_cleaned.to_csv(output_file_path, index=False)\n",
    "\n",
    "# 输出删除 \\N 后的数据大小\n",
    "original_size = data.shape[0]\n",
    "new_size = data_cleaned.shape[0]\n",
    "print(f\"原始数据行数: {original_size}\")\n",
    "print(f\"删除包含 '\\\\N' 的行后的数据行数: {new_size}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为什么用的是pt而不是ctime，因为经过检查，只有0.15%的ctime有出现第二遍，以这个来识别“天”有点没有意义。但是ctime是肯定有重复项的。（显然，一天不能只有一单吧。。。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出现不止一次的 pt 数量: 6915994\n",
      "占总数据比例: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载数据\n",
    "data_path = '流水数据/clean.txt'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 统计每个 ctime 出现的次数\n",
    "ctime_counts = data['pt'].value_counts()\n",
    "\n",
    "# 筛选出出现次数超过一次的 ctime\n",
    "multiple_ctimes = ctime_counts[ctime_counts > 1]\n",
    "\n",
    "# 计算出现多次的 ctime 数量\n",
    "multiple_ctimes_count = multiple_ctimes.sum()\n",
    "\n",
    "# 计算总数据条数\n",
    "total_data_count = data.shape[0]\n",
    "\n",
    "# 计算占总数据的比例\n",
    "multiple_ctimes_percentage = (multiple_ctimes_count / total_data_count) * 100\n",
    "\n",
    "print(f\"出现不止一次的 pt 数量: {multiple_ctimes_count}\")\n",
    "print(f\"占总数据比例: {multiple_ctimes_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始流水数据行数: 6915994\n",
      "处理后的流水数据行数: 3422391\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载数据\n",
    "stream_data_path = '流水数据/clean.txt'\n",
    "train_both_path = '训练1/merchant_in_both_datasets.txt'\n",
    "train_combined_path = '训练1/merchant_only_in_combined.txt'\n",
    "test_both_path = '测试1/test_merchant_in_both_datasets.txt'\n",
    "test_combined_path = '测试1/test_merchant_only_in_combined.txt'\n",
    "\n",
    "stream_data = pd.read_csv(stream_data_path)\n",
    "train_both = pd.read_csv(train_both_path)\n",
    "train_combined = pd.read_csv(train_combined_path)\n",
    "test_both = pd.read_csv(test_both_path)\n",
    "test_combined = pd.read_csv(test_combined_path)\n",
    "\n",
    "# 合并四个文件中的merchant_id为一个集合\n",
    "merchant_ids = pd.concat([train_both['merchant_id'], train_combined['merchant_id'], \n",
    "                          test_both['merchant_id'], test_combined['merchant_id']]).unique()\n",
    "\n",
    "# 保留流水数据中的merchant_id如果出现在任意一个文件中\n",
    "stream_data_cleaned = stream_data[stream_data['merchant_id'].isin(merchant_ids)]\n",
    "\n",
    "# 保存处理后的数据\n",
    "stream_data_cleaned.to_csv(stream_data_path, index=False)\n",
    "\n",
    "# 打印信息\n",
    "print(f\"原始流水数据行数: {len(stream_data)}\")\n",
    "print(f\"处理后的流水数据行数: {len(stream_data_cleaned)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_both_his2.txt: Total Merchant IDs: 166, Missing in clean.txt: 0\n",
      "train_combined_with_merchant_info_with_store_count.txt: Total Merchant IDs: 942, Missing in clean.txt: 0\n",
      "test_both_his2.txt: Total Merchant IDs: 67, Missing in clean.txt: 0\n",
      "test_combined_with_merchant_info_with_store_count.txt: Total Merchant IDs: 421, Missing in clean.txt: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "train_both_path = '训练1/merchant_in_both_datasets.txt'\n",
    "train_combined_path = '训练1/merchant_only_in_combined.txt'\n",
    "test_both_path = '测试1/test_merchant_in_both_datasets.txt'\n",
    "test_combined_path = '测试1/test_merchant_only_in_combined.txt'\n",
    "clean_data_path = '流水数据/clean.txt'\n",
    "\n",
    "# 读取流水数据中的 merchant_id\n",
    "clean_data = pd.read_csv(clean_data_path)\n",
    "clean_merchant_ids = set(clean_data['merchant_id'].unique())\n",
    "\n",
    "# 读取并检查文件中的 merchant_id\n",
    "def check_merchant_ids(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    merchant_ids = set(data['merchant_id'].unique())\n",
    "    total_ids = len(merchant_ids)\n",
    "    missing_ids = merchant_ids - clean_merchant_ids\n",
    "    missing_count = len(missing_ids)\n",
    "    return total_ids, missing_count\n",
    "\n",
    "# 检查每个文件\n",
    "train_both_total, train_both_missing = check_merchant_ids(train_both_path)\n",
    "train_combined_total, train_combined_missing = check_merchant_ids(train_combined_path)\n",
    "test_both_total, test_both_missing = check_merchant_ids(test_both_path)\n",
    "test_combined_total, test_combined_missing = check_merchant_ids(test_combined_path)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"train_both_his2.txt: Total Merchant IDs: {train_both_total}, Missing in clean.txt: {train_both_missing}\")\n",
    "print(f\"train_combined_with_merchant_info_with_store_count.txt: Total Merchant IDs: {train_combined_total}, Missing in clean.txt: {train_combined_missing}\")\n",
    "print(f\"test_both_his2.txt: Total Merchant IDs: {test_both_total}, Missing in clean.txt: {test_both_missing}\")\n",
    "print(f\"test_combined_with_merchant_info_with_store_count.txt: Total Merchant IDs: {test_combined_total}, Missing in clean.txt: {test_combined_missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "流水数据 'clean.txt' 中不同的 merchant_id 数量: 1586\n",
      "四个文件中不同的 merchant_id 总数: 1586\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "clean_data_path = '流水数据/clean.txt'\n",
    "train_both_path = '训练1/merchant_in_both_datasets.txt'\n",
    "train_combined_path = '训练1/merchant_only_in_combined.txt'\n",
    "test_both_path = '测试1/test_merchant_in_both_datasets.txt'\n",
    "test_combined_path = '测试1/test_merchant_only_in_combined.txt'\n",
    "\n",
    "# 读取流水数据中的 merchant_id\n",
    "clean_data = pd.read_csv(clean_data_path)\n",
    "clean_merchant_ids = set(clean_data['merchant_id'].unique())\n",
    "print(f\"流水数据 'clean.txt' 中不同的 merchant_id 数量: {len(clean_merchant_ids)}\")\n",
    "\n",
    "# 读取并汇总所有文件中的 merchant_id\n",
    "files = [train_both_path, train_combined_path, test_both_path, test_combined_path]\n",
    "all_merchant_ids = set()\n",
    "\n",
    "for file in files:\n",
    "    data = pd.read_csv(file)\n",
    "    all_merchant_ids.update(data['merchant_id'].unique())\n",
    "\n",
    "print(f\"四个文件中不同的 merchant_id 总数: {len(all_merchant_ids)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
